{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f70fa4c",
   "metadata": {},
   "source": [
    "#### Custom TokenizerV1 without handling unknown vocabulary tokens and <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc06a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the CustomTokenizer class \n",
    "\n",
    "import re\n",
    "\n",
    "class CustomTokenizerV1:\n",
    "\n",
    "    \"\"\"\n",
    "    Custom tokenizer class \n",
    "    \"\"\"\n",
    "    def __init__(self, text:str):        \n",
    "        self.tokens_to_ids = {}\n",
    "        self.ids_to_tokens = {}\n",
    "        \n",
    "        # self.text = text.lower()\n",
    "        self.special_chars = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "        r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "\n",
    "        processed_text = re.split(self.special_chars, text)\n",
    "        print(\"from init\", processed_text)\n",
    "        self.tokens = [text.strip() for text in processed_text if text.strip()]\n",
    "\n",
    "        self.tokens_to_ids = {token:idx for idx, token in enumerate(self.tokens)}\n",
    "        self.ids_to_tokens = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "\n",
    "\n",
    "    def encode(self, text:str):\n",
    "        \"\"\"\n",
    "        encodes the text into tokens and updates the instance parameters with tokens_to_ids, ids_to_tokens \n",
    "\n",
    "        Args:\n",
    "            text(str): string to tokenize\n",
    "        \n",
    "        Returns:\n",
    "            token_ids(list): list of token_ids\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "\n",
    "            processed_text = re.split(self.special_chars, text)\n",
    "            tokens = [text.strip() for text in processed_text if text.strip()]\n",
    "\n",
    "            token_ids = [self.tokens_to_ids[token] for token in tokens]\n",
    "            return token_ids\n",
    "\n",
    "        except Exception as exe:\n",
    "            print(f\"exception in encode: {exe.__str__()}\")\n",
    "            raise exe\n",
    "        \n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        decodes the list of tokens back to string \n",
    "\n",
    "        Args:\n",
    "            tokens_ids(list): list of token_ids \n",
    "\n",
    "        Returns:\n",
    "            decoded text(str) \n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens = [self.ids_to_tokens[token] for token in token_ids]\n",
    "            decoded_text = []\n",
    "            for token in tokens:\n",
    "                if token in self.special_chars: \n",
    "                    if decoded_text:  \n",
    "                        decoded_text[-1] += token\n",
    "                    else:\n",
    "                        decoded_text.append(token)\n",
    "                else:\n",
    "                    decoded_text.append(token)\n",
    "\n",
    "            return \" \".join(decoded_text)\n",
    "\n",
    "        except Exception as exe:\n",
    "            print(f\"except occured in decode: {exe.__str__()}\")\n",
    "            raise exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog, this\"\n",
    "tokenizer = CustomTokenizerV1(text=text)\n",
    "\n",
    "print(f\"original Text: {text}\")\n",
    "\n",
    "token_ids = tokenizer.encode(text)\n",
    "print(f\"token_ids: {token_ids}\")\n",
    "\n",
    "\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "print(f\"decoded text: {decoded_text}\")\n",
    "print(f\"tokens: {list(tokenizer.tokens_to_ids.keys())}\")\n",
    "\n",
    "print(tokenizer.tokens_to_ids)\n",
    "assert text==decoded_text, \"both text are not same\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8097d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to decode the text that is not available in the vocabulary\n",
    "\n",
    "print(tokenizer.decode([3,4,7,8,9]))\n",
    "print(tokenizer.encode(text=\"this brown fox\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf7201",
   "metadata": {},
   "source": [
    "#### Custom TokenizerV2 handling unknown vocabulary (<|unk|>) and <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dd476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the CustomTokenizer class \n",
    "\n",
    "import re\n",
    "\n",
    "class CustomTokenizerV2:\n",
    "\n",
    "    \"\"\"\n",
    "    Custom tokenizer class handling unknown vocabulary and endoftext tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, text:str):\n",
    "               \n",
    "        self.tokens_to_ids = {}\n",
    "        self.ids_to_tokens = {}\n",
    "        \n",
    "\n",
    "        # self.text = text.lower()\n",
    "        self.special_chars = r'([,.:;?_!\"()\\']|--|\\s)'\n",
    "        processed_text = re.split(self.special_chars, text)\n",
    "        print(\"processed text: \", processed_text)\n",
    "        self.tokens = [token.strip() for token in processed_text if token.strip()]\n",
    "        \n",
    "        print(\"from init\", self.tokens)\n",
    "\n",
    "        self.tokens_to_ids = {token:idx for idx, token in enumerate(self.tokens)}\n",
    "        self.ids_to_tokens = {idx: token for idx, token in enumerate(self.tokens)}\n",
    "\n",
    "        self.tokens_to_ids['<|unk|>'] = -100\n",
    "        self.ids_to_tokens[-100] = '<|unk|>'\n",
    "\n",
    "        # self.tokens_to_ids['<|endoftext|>'] = -200\n",
    "        # self.ids_to_tokens[-200] = '<|endoftext|>'\n",
    "\n",
    "\n",
    "    def encode(self, text:str):\n",
    "        \"\"\"\n",
    "        encodes the text into tokens and updates the instance parameters with tokens_to_ids, ids_to_tokens \n",
    "\n",
    "        Args:\n",
    "            text(str): string to tokenize\n",
    "        \n",
    "        Returns:\n",
    "            token_ids(list): list of token_ids\n",
    "        \"\"\"\n",
    "        try:\n",
    "\n",
    "            processed_text = re.split(self.special_chars, text)\n",
    "            tokens = [text.strip() for text in processed_text if text.strip()]\n",
    "\n",
    "            token_ids = [self.tokens_to_ids[token] if token in self.tokens else self.tokens_to_ids['<|unk|>'] for token in tokens]\n",
    "            return token_ids\n",
    "\n",
    "        except Exception as exe:\n",
    "            print(f\"exception in encode: {exe.__str__()}\")\n",
    "            raise exe\n",
    "        \n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        decodes the list of tokens back to string \n",
    "\n",
    "        Args:\n",
    "            tokens_ids(list): list of token_ids \n",
    "\n",
    "        Returns:\n",
    "            decoded text(str) \n",
    "        \"\"\"\n",
    "        try:\n",
    "            tokens = [self.ids_to_tokens[token] for token in token_ids]\n",
    "            decoded_text = []\n",
    "            for token in tokens:\n",
    "                if token in self.special_chars: \n",
    "                    if decoded_text:  \n",
    "                        decoded_text[-1] += token\n",
    "                    else:\n",
    "                        decoded_text.append(token)\n",
    "                else:\n",
    "                    decoded_text.append(token)\n",
    "\n",
    "            return \" \".join(decoded_text)\n",
    "\n",
    "        except Exception as exe:\n",
    "            print(f\"except occured in decode: {exe.__str__()}\")\n",
    "            raise exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5284d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "with open(\"/home/maheshbabu/LLMArchitecture/wikipedia_ai.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# replace \\n with <|endoftext|> \n",
    "text = text.replace(\"\\n\", \" <|endoftext|> <|beginoftext|> \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cd319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = CustomTokenizerV2(text)\n",
    "\n",
    "print(tokenizer.tokens)\n",
    "print(tokenizer.tokens_to_ids)\n",
    "print(tokenizer.encode(\"this is another day called <|endoftext|>\"))\n",
    "print(tokenizer.decode([2,5,6,-100]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9ff46e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text: The tiger is the beautiful animal, but it is very dangerous in the forest. Who is maheshbabu, This is demo lecture from the vizuara\n",
      "token ids: [464, 26241, 318, 262, 4950, 5044, 11, 475, 340, 318, 845, 4923, 287, 262, 8222, 13, 5338, 318, 17266, 956, 71, 65, 397, 84, 11, 770, 318, 13605, 19143, 422, 262, 410, 47775, 3301]\n",
      "decoded text: The tiger is the beautiful animal, but it is very dangerous in the forest. Who is maheshbabu, This is demo lecture from the vizuara\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"The tiger is the beautiful animal, but it is very dangerous in the forest. Who is maheshbabu, This is demo lecture from the vizuara\"\n",
    "\n",
    "print(f\"original text: {sample_text}\")\n",
    "print(f\"token ids: {tokenizer.encode(sample_text)}\")\n",
    "print(f\"decoded text: {tokenizer.decode(tokenizer.encode(sample_text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05168e8b",
   "metadata": {},
   "source": [
    "#### Using tiktoken, Dataset, DataLoader creating input-output pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cab83957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tiktoken\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8727, 318, 17266, 956, 71, 65, 397, 84]\n",
      "what is my name\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(text=\"who is maheshbabu\", allowed_special={\"<|endoftext|>\", \"<|beginoftext|>\"}))\n",
    "print(tokenizer.decode([10919, 318, 616, 1438]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4eada0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids: [220, 198, 6170, 11583, 5055, 267, 10473, 355, 339, 14170, 11448, 1785, 287, 8437, 13, 198, 10364, 3759, 1301, 5982, 284, 257, 5055, 267, 10473, 290, 34550, 422, 257, 4315, 286, 4138, 11969, 257, 11448, 1785, 319, 3909, 1755, 11, 17275, 2832, 351, 5941, 1028, 257, 26373, 286, 3296, 25849, 465, 16028, 28263, 32, 23910, 13, 198, 5703, 355, 1301, 5982, 11, 339, 21272, 9905, 2583, 5689, 8041, 272, 11, 508, 3332, 284, 262, 826, 286, 262, 1893, 13, 198, 2202, 262, 584, 1735, 286, 1301, 3332, 32451, 20119, 11, 18828, 290, 4039, 286, 262, 2732, 286, 5070, 45728, 13, 198, 6170, 11, 508, 697, 4714, 465, 3223, 6050, 351, 257, 6016, 7872, 9839, 11, 29104, 465, 18606, 287, 262, 1633, 11, 21550, 34550, 284, 21245, 286, 564, 250, 26556, 7276, 286, 7320, 447, 251, 13, 198]\n",
      "input ids:  [tensor([  220,   198,  6170, 11583,  5055]), tensor([ 5055,   267, 10473,   355,   339]), tensor([  339, 14170, 11448,  1785,   287]), tensor([  287,  8437,    13,   198, 10364]), tensor([10364,  3759,  1301,  5982,   284]), tensor([  284,   257,  5055,   267, 10473]), tensor([10473,   290, 34550,   422,   257]), tensor([  257,  4315,   286,  4138, 11969]), tensor([11969,   257, 11448,  1785,   319]), tensor([  319,  3909,  1755,    11, 17275]), tensor([17275,  2832,   351,  5941,  1028]), tensor([ 1028,   257, 26373,   286,  3296]), tensor([ 3296, 25849,   465, 16028, 28263]), tensor([28263,    32, 23910,    13,   198]), tensor([ 198, 5703,  355, 1301, 5982]), tensor([ 5982,    11,   339, 21272,  9905]), tensor([9905, 2583, 5689, 8041,  272]), tensor([ 272,   11,  508, 3332,  284]), tensor([284, 262, 826, 286, 262]), tensor([ 262, 1893,   13,  198, 2202]), tensor([2202,  262,  584, 1735,  286]), tensor([  286,  1301,  3332, 32451, 20119]), tensor([20119,    11, 18828,   290,  4039]), tensor([4039,  286,  262, 2732,  286]), tensor([  286,  5070, 45728,    13,   198]), tensor([ 198, 6170,   11,  508,  697]), tensor([ 697, 4714,  465, 3223, 6050]), tensor([6050,  351,  257, 6016, 7872]), tensor([ 7872,  9839,    11, 29104,   465]), tensor([  465, 18606,   287,   262,  1633]), tensor([ 1633,    11, 21550, 34550,   284]), tensor([  284, 21245,   286,   564,   250]), tensor([  250, 26556,  7276,   286,  7320])]\n",
      "target ids:  [tensor([  198,  6170, 11583,  5055,   267]), tensor([  267, 10473,   355,   339, 14170]), tensor([14170, 11448,  1785,   287,  8437]), tensor([ 8437,    13,   198, 10364,  3759]), tensor([3759, 1301, 5982,  284,  257]), tensor([  257,  5055,   267, 10473,   290]), tensor([  290, 34550,   422,   257,  4315]), tensor([ 4315,   286,  4138, 11969,   257]), tensor([  257, 11448,  1785,   319,  3909]), tensor([ 3909,  1755,    11, 17275,  2832]), tensor([2832,  351, 5941, 1028,  257]), tensor([  257, 26373,   286,  3296, 25849]), tensor([25849,   465, 16028, 28263,    32]), tensor([   32, 23910,    13,   198,  5703]), tensor([5703,  355, 1301, 5982,   11]), tensor([   11,   339, 21272,  9905,  2583]), tensor([2583, 5689, 8041,  272,   11]), tensor([  11,  508, 3332,  284,  262]), tensor([ 262,  826,  286,  262, 1893]), tensor([1893,   13,  198, 2202,  262]), tensor([ 262,  584, 1735,  286, 1301]), tensor([ 1301,  3332, 32451, 20119,    11]), tensor([   11, 18828,   290,  4039,   286]), tensor([ 286,  262, 2732,  286, 5070]), tensor([ 5070, 45728,    13,   198,  6170]), tensor([6170,   11,  508,  697, 4714]), tensor([4714,  465, 3223, 6050,  351]), tensor([ 351,  257, 6016, 7872, 9839]), tensor([ 9839,    11, 29104,   465, 18606]), tensor([18606,   287,   262,  1633,    11]), tensor([   11, 21550, 34550,   284, 21245]), tensor([21245,   286,   564,   250, 26556]), tensor([26556,  7276,   286,  7320,   447])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "\n",
    "class CustomDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, context_size=5, stride=4):\n",
    "\n",
    "        self.input_ids = []\n",
    "        self.target = []\n",
    "\n",
    "        token_ids = tokenizer.encode(text=txt, allowed_special={\"<|endoftext|>\"})\n",
    "        print(\"Token ids:\", token_ids)\n",
    "\n",
    "        for i in range(0, len(token_ids)-context_size, stride):\n",
    "            self.input_ids.append(torch.tensor(token_ids[i:i+context_size]))\n",
    "            self.target.append(torch.tensor(token_ids[i+1:i+context_size+1]))\n",
    "\n",
    "        print(\"input ids: \", self.input_ids)\n",
    "        print(\"target ids: \", self.target)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.input_ids[index], self.target[index]\n",
    "    \n",
    "\n",
    "sample_text = \"\"\" \n",
    "Trump receives standing ovation as he enters UFC event in Miami.\n",
    "President Donald Trump entered to a standing ovation and cheers from a crowd of thousands attending a UFC event on Saturday night, shaking hands with supporters against a backdrop of fans waving his trademark MAGA hats.\n",
    "Just as Trump entered, he greeted podcast host Joe Rogan, who sat to the right of the president.\n",
    "On the other side of Trump sat Elon Musk, billionaire and chief of the Department of Government Efficiency.\n",
    "Trump, who accented his dark suit with a bright yellow tie, pumped his fist in the air, prompting cheers to strains of “Taking Care of Business”.\n",
    "\"\"\"\n",
    "dataset = CustomDatasetV1(txt=sample_text, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bf4f1e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 5055,   267, 10473,   355,   339]),\n",
       " tensor([  267, 10473,   355,   339, 14170]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "88e5b5a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  220,   198,  6170, 11583,  5055],\n",
       "         [ 5055,   267, 10473,   355,   339],\n",
       "         [  339, 14170, 11448,  1785,   287],\n",
       "         [  287,  8437,    13,   198, 10364]]),\n",
       " tensor([[  198,  6170, 11583,  5055,   267],\n",
       "         [  267, 10473,   355,   339, 14170],\n",
       "         [14170, 11448,  1785,   287,  8437],\n",
       "         [ 8437,    13,   198, 10364,  3759]])]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataloader = DataLoader(dataset=dataset, shuffle=False, batch_size=4, drop_last=True, num_workers=2)\n",
    "\n",
    "data = iter(dataloader)\n",
    "next(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
